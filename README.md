
![image](https://github.com/user-attachments/assets/52c0785b-06b8-451f-af6a-0b6520fe13f7)

**<h1 align="center">LOPT: Advanced Deepfake Classification Framework</h1>**
<div align="center">

[![Python](https://img.shields.io/badge/Python-3.12%2B-blue)](https://www.python.org/)
[![React](https://img.shields.io/badge/React-19.0.0-61dafb)](https://reactjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Latest-009688)](https://fastapi.tiangolo.com/)
[![JavaScript](https://img.shields.io/badge/JavaScript-5.0.2-3178c6)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-Latest-646cff)](https://vitejs.dev/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/Transformers-4.0%2B-green)](https://huggingface.co/transformers/)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-yellow?logo=huggingface&logoColor=white)](https://huggingface.co/agasta/virtus)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/Itz-Agasta/lopt/pulls)
[![GitHub Issues](https://img.shields.io/github/issues/Itz-Agasta/lopt)](https://github.com/Itz-Agasta/lopt/issues)
[![GitHub Stars](https://img.shields.io/github/stars/Itz-Agasta/lopt)](https://github.com/Itz-Agasta/lopt/stargazers)
[![Last Commit](https://img.shields.io/github/last-commit/Itz-Agasta/lopt)](https://github.com/Itz-Agasta/lopt/commits/main)

</div>

  <p align="center">
  Lopt is a multi-modal deepfake classification framework leveraging transformer-based architectures for high-confidence media authentication. It integrates ViT and TimeSformer models for  classifications, optimized via stratified training, augmentation, and parameter-efficient fine-tuning. Built for real-world robustness, Lopt supports scalable inference through a FastAPI backend and offers a React-powered UI for streamlined interaction. 
    <br />
    <br />
    <a href="https://youtu.be/9IbQkp3FVAs">View Demo</a>
    .
    <a href="https://github.com/Itz-Agasta/lopt/issues">Report Bug</a>
    Â·
    <a href="https://github.com/Itz-Agasta/lopt/issues">Request Feature</a>
    Â·
    <a href="https://github.com/Itz-Agasta/lopt/pulls">Send a Pull Request</a>
  </p>

## What Are Deepfakes â€” and Why Should You Care?

Deepfakes are AI-generated media â€” often hyper-realistic videos or images â€” where a personâ€™s face, voice, or identity is altered or replaced with another. Initially a byproduct of generative research (e.g., GANs and diffusion models), deepfakes have rapidly evolved from novelty to existential threat.

> ðŸ“‰ Implications: Misinformation, digital harassment, political manipulation, identity theft, and erosion of public trust.

![image](https://github.com/user-attachments/assets/2f2e248e-d211-4d1d-818b-c3882f61eb78)
*<p align="center">A screenshot comparing a real video of President Barrack Obama with a deepfaked version, Jul. 2019 [Youtube/UC Berkeley](/https://youtu.be/51uHNgmnLWI/CC)</p>*

As synthetic content becomes increasingly indistinguishable from authentic media, we need machine learning models that can see what humans no longer can.

## Meet Lopt

Lopt is a multimodal deepfake detection framework designed to analyze and classify images and videos as `Real` or `Fake`. It combines:

- **ViT-based** architectures for image classification

- **TimeSformer-based** models for video understanding

- Clean and reproducible pipelines for training, evaluation, and deployment

- Full-stack implementation for both research and real-world use


https://github.com/user-attachments/assets/e41fa811-eb4d-478b-a12f-4e775d59b78b


  

## Project Overview

This project implements two complementary models for deepfake detection:

1. **Virtus**: A Vision Transformer (ViT) based model for image-level deepfake detection with binary classification head.

   - `Base Model:` [facebook/deit-base-distilled-patch16-224](https://arxiv.org/abs/2012.12877)
   - `Dataset:` [~190k labeled samples across real/fake images](https://drive.google.com/drive/folders/1UksPJfRtgw_fLsGcpCRAEWSyt_5fSA9Y?usp=sharing)
   - `Results:`
     - Accuracy: 99.20%
     - Macro F1 Score: 0.9920
     - Evaluated using classification reports and confusion matrix
   - Inference Ready: Deployed to Hugging Face ðŸ¤— [agasta/virtus](https://huggingface.co/agasta/virtus)

2. **Scarlet**: A TimeSformer-based architecture for video-level deepfake analysis with memory-efficient Temporal attention mechanism.

- `Base Model:` [facebook/timesformer-base-finetuned-k400](https://huggingface.co/facebook/timesformer-base-finetuned-k400)
- `Dataset:` [FaceForensics++](https://github.com/ondyari/FaceForensics)
- `Results:` - Accuracy: 96% - Macro F1 Score: 0.958 - Evaluated using classification reports and confusion matrix
- Inference Ready: Deployed to Hugging Face ðŸ¤— [agasta/scarlet](https://huggingface.co/agasta/scarlet)

![image](https://github.com/user-attachments/assets/a8ec502f-8ff3-4409-a0c3-c1a22ec2f774)
*<p align="center">Comparative evaluation of deepfake detection performance across modern and legacy models.</p>*

The framework is designed to be modular, allowing for easy integration of new models, datasets, evaluation metrics or with other projects.
You can load the model into your project using:

```python
from transformers import AutoModelForImageClassification, AutoFeatureExtractor

model = AutoModelForImageClassification.from_pretrained("agasta/virtus")
extractor = AutoFeatureExtractor.from_pretrained("agasta/virtus")
```

It also provides a user-friendly interface for uploading and analyzing media files. For full training code, dataset
preprocessing, and evaluation metrics, check the [Virtus model README](./models/image/README.md)
& [Scarlet model README](./models/video/README.md)

## Architecture

Lopt is structured as a modular ML system built for fast inference, responsive UX, and scalable deployment.
It integrates a RESTful backend, transformer-based inference engines, and a frontend interface â€” all wired together
via clean API boundaries.

### System Overview

![diagram-export-4-20-2025-5_07_55-AM](https://github.com/user-attachments/assets/86e074e3-8502-4866-8803-9580f4bd1ef8)

*<p align="center">Fullstack System Architecture</p>*


### Request Lifecycle:

1. **Media Upload (Frontend)**

   - Built with React and TailwindCSS, Users select or drag-and-drop an image or video
   - `Axios` sends a `POST` request to `/playground`endpoint with the media as multipart/form-data

2. **Backend Routing (FastAPI)**

   - FastAPI handles the `/playground` route asynchronously
   - Payload is validated and temporarily saved or streamed in-memory
   - Inference logic branches based on file MIME type (image/, video/)

3. **Model Inference Pipeline**

   - Transformer models are pre-loaded and cached using Hugging Faceâ€™s `AutoModelForImageClassification` or `AutoModelForVideoClassification`
   - Corresponding processors handle resizing, normalization, and tensor conversion (ViTImageProcessor)
   - Inputs are batched and pushed through the model with `torch.no_grad()` to ensure fast, memory-safe execution

4. **Output Generation**

   - Model logits are passed through softmax to generate class probabilities
   - Top predicted class is mapped back using `config.id2label`
   - Output is serialized into a response payload:

   ```json
   {
     "type": "image",
     "label": "Fake",
     "confidence": 0.9912
   }
   ```

5. **Frontend Display**
   - Response is parsed and rendered in the UI with color-coded tags and animation feedback
     > Future Note: will include saliency maps, Grad-CAM visualizations, and image overlays for transparency

### Model Caching Strategy:

- Models (Virtus, Scarlet) are loaded on server startup and held in-memory via Python singleton pattern, module scope
- Avoids repeated `from_pretrained()` calls, reducing cold start latency
- Torch is placed in `eval()` mode with disabled gradients for optimal performance
- Devices auto-detect (CPU/GPU) with planned support for `ONNX` and `TensorRT acceleration`

### Tech Stack Overview:

| Layer          | Tech Used                                                                                                |
| -------------- | -------------------------------------------------------------------------------------------------------- |
| **Backend**    | Python, FastAPI, Uvicorn, Pillow, Torch-cpu, Torchvision-cpu, Transformers, Opencv                       |
| **Frontend**   | JavaScript, React, Tailwind, Vite, Framer Motion, Lenis Smooth Scroll                                    |
| **Models**     | PyTorch, Hugging Face, Transformers, Scikit-learn, Imbalanced-learn, Accelerate, Pillow, Virtus, Scarlet |
| **Training**   | Kaggle Accelerators + ðŸ¤— Hub                                                                             |
| **Management** | UV, npm, pytest, Pydantic v2, Ruff                                                                       |
| **DevOps**     | Docker, GitHub Actions, GCP, AWS                                                                         |

## Repository Structure

```bash
lopt/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/                # Static files
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ pages/             # Page-level components
â”‚   â”‚   â”œâ”€â”€ hooks/             # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/          # API service layer
â”‚   â”‚   â”œâ”€â”€ styles/            # CSS/styled-components
â”‚   â”‚   â”œâ”€â”€ App.jsx            # Main app logic
â”‚   â”‚   â””â”€â”€ index.jsx          # Entry point
â”‚   â”œâ”€â”€ .env                   # Frontend environment config
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md

â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/     # Route handlers (upload.py, job.py, health.py)
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ main.py            # Entry point for FastAPI app
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ tests/                 # Backend tests
â”‚   â”œâ”€â”€ Dockerfile             # Backend container definition
â”‚   â”œâ”€â”€ Pyproject.toml         # Python dependencies
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Virtus              # Image-based deepfake detection model
â”‚   â”œâ”€â”€ Scarlet             # Video-based deepfake detection model
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## Installation Guide

<details>
  <summary>Prerequisites</summary>

- **Python**: Ensure Python 3.12 or higher is installed. You can download it from [python.org](https://www.python.org/downloads/).
- **Node.js**: Install Node.js (v19.0.0 or higher) and npm for the frontend.
- **[UV](https://docs.astral.sh/uv/getting-started/installation/)**: Ensure UV is installed for managing Python dependencies.
- **Git**: Install Git to clone the repository.
- **Docker**: For easy container builds.

</details>

### Step 1: Clone the Repository

```bash
git clone https://github.com//Itz-Agasta/lopt.git
cd lopt
```

### Step 2: Set Up the Frontend

1.  Install Frontend Dependencies:

```bash
cd frontend
npm install
```

2.  Build the Frontend:

```bash
npm run build
```

### Step 3: Set Up the Backend

1.  Install Python Dependencies:

```bash
cd backend
uv sync
```

### Step 4: Run the Application

1.  Start the Backend:

```bash
uv run fastapi dev
```

2. Start the Frontend:

```bash
npm run dev
```

3. Access the application at http://localhost:3000.

### Step 5: Verify Installation

- Test the backend by visiting http://localhost:8000/docs for the FastAPI Swagger UI.
- Test the frontend by uploading an image or video for classification.

## Contributors

<a href="https://github.com/Itz-Agasta/lopt/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Itz-Agasta/lopt" />
</a>

## ðŸ“š Citations

Lopt draws inspiration from key foundational papers in the deepfake detection space:

- ðŸ“„ **FaceForensics++**  
  _Rossler et al., ICCV 2019_  
  [https://arxiv.org/abs/1901.08971](https://arxiv.org/abs/1901.08971)

- ðŸ“„ **Exposing DeepFakes Using Inconsistent Head Poses**  
  _Yang et al., CVPRW 2019_  
  [https://arxiv.org/abs/1811.00661](https://arxiv.org/abs/1811.00661)

- ðŸ“„ **TimeSformer: Attention Is All You Need for Video Understanding**  
  _Bertasius et al., 2021_  
  [https://arxiv.org/abs/2102.05095](https://arxiv.org/abs/2102.05095)

- ðŸ“„ **Training Vision Transformers for Deepfake Detection**  
  _Nasrullah et al., NeurIPS 2022_  
  [https://arxiv.org/abs/2210.08174](https://arxiv.org/abs/2210.08174)

## License

This project is licensed under the [MIT License](https://github.com/Itz-Agasta/lopt/blob/main/License). See the LICENSE file for more details.

Feel free to Send a [Pull Request](https://github.com/Itz-Agasta/lopt/pulls) if you have improvements or fixes.
For questions or feedback, please open an issue or contact [rupam.golui@proton.me](mailto:rupam.golui@proton.me)
